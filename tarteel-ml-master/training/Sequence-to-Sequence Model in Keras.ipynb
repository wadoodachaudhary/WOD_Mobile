{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import dill as pickle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Preprocess Audio Files in Surah Fatihah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take quite a bit of time, but the good news is that you only need to do it once! After you've done this once, the files will be saved locally and you can skip the cells in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../download.py\" -s 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i \"../audio_preprocessing/generate_features.py\" -f mfcc -s 1 --local_download_dir \"../.audio\" --output_dir \"../.outputs\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../audio_preprocessing/generate_one_hot_encoding.py\" -i \"../data/data-uthmani.json\" -o \"../data/one-hot.pkl\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Methods to Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\"\"\"\n",
    "\n",
    "def convert_list_of_arrays_to_padded_array(list_varying_sizes, pad_value=0):\n",
    "    '''\n",
    "    Converts a list of arrays of varying sizes to a single numpy array. The extra elements are set to 0\n",
    "    '''\n",
    "    max_shape = [0]*len(list_varying_sizes[0].shape)\n",
    "    # first pass to compute the max size\n",
    "    for arr in list_varying_sizes:\n",
    "        shape = arr.shape\n",
    "        max_shape = [max(s1, s2) for s1, s2 in zip(shape, max_shape)]\n",
    "    padded_array = pad_value * np.ones((len(list_varying_sizes), *max_shape))\n",
    "    \n",
    "    # second pass to fill in the values in the array:\n",
    "    for a, arr in enumerate(list_varying_sizes):\n",
    "        r, c = arr.shape  # TODO(abidlabs): maybe make more general to more than just 2D arrays.\n",
    "        padded_array[a, :r, :c] = arr\n",
    "    \n",
    "    return padded_array\n",
    "\n",
    "def preprocess_encoder_input(arr):\n",
    "    '''\n",
    "    Simple method to handle the complex MFCC coefs that are produced during preprocessing. This means:\n",
    "    1. (For now), discarding one of the channels of the MFCC coefs\n",
    "    2. Collapsing any empty dimensions\n",
    "    '''\n",
    "    return arr.squeeze()[0]\n",
    "\n",
    "\n",
    "# Load every one-hot-encoded output as a dictionary\n",
    "with open('../data/one-hot.pkl', 'rb') as one_hot_quran_pickle_file:\n",
    "    one_hot_obj = pickle.load(one_hot_quran_pickle_file)\n",
    "\n",
    "\n",
    "def get_one_hot_encoded_verse(surah_num, ayah_num): \n",
    "    '''\n",
    "    Converts a one-hot-encoded verse into forms that can be used by the LSTM decoder\n",
    "    \n",
    "    :param surah_num: an int designating the chapter number, one-indexed\n",
    "    :param ayah_num: an int designating the verse number, one-indexed\n",
    "    '''\n",
    "    # Load the preprocessed one-hot encoding \n",
    "    one_hot_verse = one_hot_obj['quran']['surahs'][surah_num - 1]['ayahs'][ayah_num - 1]['text']\n",
    "    num_chars_in_verse, num_unique_chars = one_hot_verse.shape\n",
    "    \n",
    "    # Generate decoder_input_data \n",
    "    decoder_input = np.zeros((num_chars_in_verse+2, num_unique_chars+2))\n",
    "    decoder_input[0, :] = [0] * num_unique_chars + [1, 0] # START token\n",
    "    decoder_input[1:num_chars_in_verse+1, :-2] = one_hot_verse # original verse\n",
    "    decoder_input[-1, :] = [0] * num_unique_chars + [0, 1] # STOP token\n",
    "\n",
    "    # Generate decoder_target_data \n",
    "    decoder_target = np.zeros((num_chars_in_verse+2, num_unique_chars+2))\n",
    "    decoder_target[:num_chars_in_verse, :-2] = one_hot_verse # original verse\n",
    "    decoder_target[-2, :] = [0] * num_unique_chars + [0, 1] # STOP token\n",
    "    \n",
    "    return decoder_input, decoder_target\n",
    "\n",
    "    \n",
    "def build_dataset(local_coefs_dir='../.outputs/mfcc', surahs=[1], n=100):\n",
    "    '''\n",
    "    Builds a dataset to be used with the sequence-to-sequence network.\n",
    "    \n",
    "    :param local_coefs_dir: a string with the path of the coefficients for prediction\n",
    "    '''\n",
    "    \n",
    "    def get_encoder_and_decoder_data(n=100):\n",
    "        count = 0\n",
    "        encoder_input_data = []\n",
    "        decoder_input_data = []\n",
    "        decoder_target_data = []\n",
    "        for surah_num in surahs:\n",
    "            local_surah_dir = os.path.join(local_coefs_dir, \"s\" + str(surah_num))\n",
    "            for _, ayah_directories, _ in os.walk(local_surah_dir):\n",
    "                for ayah_directory in ayah_directories:\n",
    "                    ayah_num = ayah_directory[1:]\n",
    "                    local_ayah_dir = os.path.join(local_surah_dir, ayah_directory)\n",
    "                    for _, _, recording_filenames in os.walk(local_ayah_dir):\n",
    "                        for recording_filename in recording_filenames:\n",
    "                            local_coefs_path = os.path.join(local_ayah_dir, recording_filename)\n",
    "                            encoder_input = np.load(local_coefs_path)\n",
    "                            encoder_input = preprocess_encoder_input(encoder_input)\n",
    "                            encoder_input_data.append(encoder_input)\n",
    "\n",
    "                            decoder_input, decoder_target = get_one_hot_encoded_verse(int(surah_num), int(ayah_num))\n",
    "                            decoder_input_data.append(decoder_input)\n",
    "                            decoder_target_data.append(decoder_target)\n",
    "                            count += 1\n",
    "                            if count == n:\n",
    "                                return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "        return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "    \n",
    "    \n",
    "    encoder_input_data, decoder_input_data, decoder_target_data = get_encoder_and_decoder_data(n=n)\n",
    "    encoder_input_data = convert_list_of_arrays_to_padded_array(encoder_input_data)\n",
    "    decoder_input_data = convert_list_of_arrays_to_padded_array(decoder_input_data)\n",
    "    decoder_target_data = convert_list_of_arrays_to_padded_array(decoder_target_data)\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # Batch size for training.\n",
    "epochs = 25  # Number of epochs to train for.\n",
    "latent_dim = 10  # Latent dimensionality of the encoding space.\n",
    "n = 100\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = build_dataset(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1067, 13)\n",
      "(100, 40, 61)\n",
      "(100, 40, 61)\n"
     ]
    }
   ],
   "source": [
    "[print(a.shape) for a in [encoder_input_data, decoder_input_data, decoder_target_data]]\n",
    "\n",
    "max_encoder_seq_length = encoder_input_data.shape[1]\n",
    "max_decoder_seq_length = decoder_input_data.shape[1]\n",
    "num_encoder_tokens = encoder_input_data.shape[-1]\n",
    "num_decoder_tokens = decoder_input_data.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Keras Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/25\n",
      "80/80 [==============================] - 12s 144ms/step - loss: 3.5909 - val_loss: 2.4462\n",
      "Epoch 2/25\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 3.5590 - val_loss: 2.4280\n",
      "Epoch 3/25\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 3.5232 - val_loss: 2.4028\n",
      "Epoch 4/25\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 3.4681 - val_loss: 2.3597\n",
      "Epoch 5/25\n",
      "80/80 [==============================] - 8s 98ms/step - loss: 3.3633 - val_loss: 2.2845\n",
      "Epoch 6/25\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 3.2324 - val_loss: 2.2125\n",
      "Epoch 7/25\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 3.1318 - val_loss: 2.1556\n",
      "Epoch 8/25\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 3.0533 - val_loss: 2.1089\n",
      "Epoch 9/25\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 2.9862 - val_loss: 2.0692\n",
      "Epoch 10/25\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 2.9259 - val_loss: 2.0341\n",
      "Epoch 11/25\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 2.8704 - val_loss: 2.0023\n",
      "Epoch 12/25\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 2.8186 - val_loss: 1.9734\n",
      "Epoch 13/25\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.7700 - val_loss: 1.9464\n",
      "Epoch 14/25\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 2.7241 - val_loss: 1.9212\n",
      "Epoch 15/25\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.6810 - val_loss: 1.8980\n",
      "Epoch 16/25\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 2.6404 - val_loss: 1.8767\n",
      "Epoch 17/25\n",
      "80/80 [==============================] - 8s 96ms/step - loss: 2.6030 - val_loss: 1.8578\n",
      "Epoch 18/25\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.5681 - val_loss: 1.8412\n",
      "Epoch 19/25\n",
      "80/80 [==============================] - 7s 92ms/step - loss: 2.5364 - val_loss: 1.8264\n",
      "Epoch 20/25\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 2.5080 - val_loss: 1.8140\n",
      "Epoch 21/25\n",
      "80/80 [==============================] - 7s 86ms/step - loss: 2.4827 - val_loss: 1.8044\n",
      "Epoch 22/25\n",
      "80/80 [==============================] - 8s 97ms/step - loss: 2.4607 - val_loss: 1.7967\n",
      "Epoch 23/25\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 2.4421 - val_loss: 1.7899\n",
      "Epoch 24/25\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.4262 - val_loss: 1.7863\n",
      "Epoch 25/25\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 2.4128 - val_loss: 1.7812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23bb06c6e80>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), history.history['val_loss'])\n",
    "plt.plot(range(epochs), history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drop in loss curves suggest that the model is learning something. At this point, it hasn't overfit to the validation set, likely because our model is too simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_target_char_index = one_hot_obj['int_to_char']\n",
    "reverse_target_char_index[num_decoder_tokens-2] = '->'\n",
    "reverse_target_char_index[num_decoder_tokens-1] = '<-'\n",
    "\n",
    "target_char_index = {v: k for k, v in reverse_target_char_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n",
      "Predicted verse: ٱٱللِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِِ\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_char_index['->']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<-' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Predicted verse:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
